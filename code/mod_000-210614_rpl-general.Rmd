---
title: "Modelo general para R-Markdown con Python y LaTeX"
author: "Teodoro J. Martínez Arán"
date: "14/6/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library (reticulate)
library (knitr)
library (kableExtra)
library (tidyverse)
library (lubridate)
library (magrittr)  # %<>%

pd <- import("pandas") # Importa la biblioteca 'Pandas' de Python
np <- import("numpy") # Importa la biblioteca 'Numpy' de Python

```

# Fuentes de los datos
La estructura y parte de los contenidos de este modelo derivan del [curso de Udemy de Juan Gabriel Gomila Salas para análisis de datos del Coronavirus](https://www.udemy.com/course/covid19-r/)

Los datos de este documento provienen de la compilación hecha por usuarios de [Kaggle](https://www.kaggle.com/imdevskp/corona-virus-report). Los datos corresponden a la versión 166, a fecha 15-Jun-2021.


# Carga y limpieza inicial de los datos

```{r Carga y limpieza inicial de datos}
data_path <- c("../../../../Alacena/data/COVID19/covid_19_clean_complete.csv")
data <- read_csv (data_path,
                  skip = 1,
                  col_names = c ("Provincia_Estado",
                                 "Pais_Region",
                                 "Latitud", # Norte(+) o Sur (-)
                                 "Longitud", # Este (+) u Oeste (-)
                                 "Fecha",
                                 "Casos_Confirmados",
                                 "Casos_Fallecidos",
                                 "Casos_Recuperados",
                                 "Casos_Activos",
                                 "Region_OMS"
                                 ),
                  col_types = c (Provincia_Estado = "f",
                                 Pais_Region = "f",
                                 Latitud = "n",
                                 Longitud = "n",
                                 Fecha = "D",
                                 Casos_Confirmados = "i",
                                 Casos_Fallecidos = "i",
                                 Casos_Recuperados = "i",
                                 Casos_Activos = "i",
                                 Region_OMS = "f"
                                 )
                  )

```
## Estructura de los datos

```{r Estructura de los datos}
data |> head(10) |> kable() |> kable_styling()

```
## Auditoría de valores anómalos

Para auditar la calidad de los valores, se pueden crear columnas calculadas a partir de los datos, que generen valores aparentemente lógicos. Si aparecen valores imposibles, alguno de los datos que participan en el cálculo contiene un error.

Como ejemplo, utilizaremos una columna calculada para los pacientes activos. Para todas las filas, debe cumplirse la siguiente igualdad:

$$ Confirmados = Muertos + Recuperados + Activos $$
Por tanto, los casos activos calculados podrían obtenerse mediante la siguiente operación:

$$ Activos = Confirmados - Fallecidos - Recuperados $$
Podemos insertar la columna calculada mediante el código siguiente:
```{r}
data %<>%
  mutate (Casos_Activos_calc = Casos_Confirmados - Casos_Fallecidos - Casos_Recuperados)
data |> tail()
```

Los casos activos no deberían ser negativos, salvo que haya un error o alguna explicación no lógica. Por tanto, cuando filtramos los datos en los que la nueva columna ofrezca datos negativos, obtendremos aquellas filas con incongruencias de datos.

```{r}
data |>
  filter (Casos_Activos_calc < 0) |>
  arrange(Provincia_Estado, Fecha) |>
  kable() |> 
  kable_styling()

```
Con estos datos anómalos identificados hay dos estrategias posibles:
1.- Si se conoce el origen de la anomalía, se puede corregir el dato erróneo
2.- Si no se conoce el origen, deben documentarse y tratarse de manera específica

# Análisis geográfico

## Delimitación de zonas geográficas amplias (rectángulos)

En primer lugar, debemos filtrar los datos geográficos que nos interesan.

En el caso de tablas de datos con longitud y latitud, podemos crear inicialmente un rectángulo que encuadre los países de interés.

En un segundo paso, eliminaremos aquellos países en los que no tenemos observaciones

```{r}
data_europa <- data |>
  filter (Latitud >38,
          between(Longitud, -25, 30)
          )
nrow (data_europa)
table(data_europa$Pais_Region) |>
  as.data.frame() |>
  filter(Freq >0)
  
```
## Filtrado de datos geográficos por radio (bolas)

Para saber los datos que están en torno a un determinado radio, a partir de un punto central, utilizaremos la distancia euclídea. Se calcula mediante la siguiente fórmula:

$$ d(x,y) = \sqrt {(x_{Lat}-y_{Lat})^2+ (x_{Long}-y_{Long})^2} $$
Definiremos una función 'distancia_grados', que calcule la fórmula:
```{r}
distancia_grados <- function (x,y){
  sqrt((x[1]-y[1])^2 + (x[2]-y[2])^2)
}
```

Y en segundo lugar, crearemos una función que calcule la distancia a una determinada localización, como por ejemplo 'distancia_grados_HCN'

```{r}
distancia_grados_hcn <- function (x){
  hcn <- c (38.104333, -1.867359)
  distancia_grados (x, hcn)
}
```
De este modo, podemos saber los distancia de los casos respecto al centro definido, aplicando esta función:

```{r}
distancia_hcn <- apply(cbind(data_europa$Latitud, data_europa$Longitud),
                       MARGIN = 1,
                       FUN = distancia_grados_hcn
                       )
```
Si añadimos la columna creada a los datos de Europa, obtendremos la distancia de los casos al HCN de todos los casos

```{r}
data_europa %<>%
  mutate (
    distancia_hcn = distancia_hcn
  )

```
Para identificar si hemos tenido muchos casos cerca del hcn en una determinada fecha, podríamos filtrar los casos:

```{r}
data_europa |>
  filter (between (Fecha,
                   dmy("01-03-2020"), 
                   dmy("17-03-2020")
                   ),
         distancia_hcn < 4
         ) |>
  kable ()
```

Los datos agregados geolocalizan falsamente los datos en un punto que se considera el centro de masas (todos los datos son iguales. Si esta función se aplica al dataset de datos crudos, podemos geolocalizar cómo se propagan las epidemias en tiempo real.